{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from rl.core import Agent\n",
    "from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
    "from rl.util import *\n",
    "from rl.memory import Memory\n",
    "# from rl.layers import Layer\n",
    "from rl.agents.dqn import AbstractDQNAgent\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList\n",
    "from keras import initializers, regularizers, activations, constraints\n",
    "# from rl.callbacks import (\n",
    "#     TrainIntervalLogger,\n",
    "#     Visualizer,\n",
    "# )\n",
    "from copy import deepcopy\n",
    "\n",
    "import operator\n",
    "import warnings\n",
    "import timeit\n",
    "import json\n",
    "from tempfile import mkdtemp\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "修改Memory成PrioritizedMemory\n",
    "#根據TD-error作為被選取到機率的依據 Reference:https://arxiv.org/abs/1511.05952\n",
    "argument:\n",
    "    alpha:priority \n",
    "    beta:importance-sampling \n",
    "    steps_annealed:Simulate Anneal\n",
    "    根據Reference的實驗結果 alpha=.6, start_beta=.4, end_beta=1. 或alpha=.7, start_beta=.5, end_beta=1.\n",
    "    這兩種為最佳組合\n",
    "\"\"\"\n",
    "Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "class PrioritizedMemory(Memory): \n",
    "    def __init__(self, limit, alpha=.6, start_beta=.4, end_beta=1., steps_annealed=10000000, **kwargs):\n",
    "        super(PrioritizedMemory, self).__init__(**kwargs)\n",
    "\n",
    "        #The capacity of the replay buffer\n",
    "        self.limit = limit\n",
    "\n",
    "        #Transitions are stored in individual RingBuffers, similar to the SequentialMemory.\n",
    "        #This does complicate things a bit relative to the OpenAI baseline implementation.\n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.terminals = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "\n",
    "        assert alpha >= 0\n",
    "        #how aggressively to sample based on TD error\n",
    "        self.alpha = alpha\n",
    "        #how aggressively to compensate for that sampling. This value is typically annealed\n",
    "        #to stabilize training as the model converges (beta of 1.0 fully compensates for TD-prioritized sampling).\n",
    "        self.start_beta = start_beta\n",
    "        self.end_beta = end_beta\n",
    "        self.steps_annealed = steps_annealed\n",
    "\n",
    "        #SegmentTrees need a leaf count that is a power of 2\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.limit:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        #Create SegmentTrees with this capacity\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        self.max_priority = 1.\n",
    "\n",
    "        #wrapping index for interacting with the trees\n",
    "        self.next_index = 0\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\\\n",
    "        #super() call adds to the deques that hold the most recent info, which is fed to the agent\n",
    "        #on agent.forward()\n",
    "        super(PrioritizedMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        if training:\n",
    "            self.observations.append(observation)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "            #The priority of each new transition is set to the maximum\n",
    "            self.sum_tree[self.next_index] = self.max_priority ** self.alpha\n",
    "            self.min_tree[self.next_index] = self.max_priority ** self.alpha\n",
    "\n",
    "            #shift tree pointer index to keep it in sync with RingBuffers\n",
    "            self.next_index = (self.next_index + 1) % self.limit\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        #outputs a list of idxs to sample, based on their priorities.\n",
    "        idxs = list()\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            mass = random.random() * self.sum_tree.sum(0, self.limit - 1)\n",
    "            idx = self.sum_tree.find_prefixsum_idx(mass)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        return idxs\n",
    "\n",
    "    def sample(self, batch_size, beta=1.):\n",
    "        idxs = self._sample_proportional(batch_size)\n",
    "\n",
    "        #importance sampling weights are a stability measure\n",
    "        importance_weights = list()\n",
    "\n",
    "        #The lowest-priority experience defines the maximum importance sampling weight\n",
    "        prob_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_importance_weight = (prob_min * self.nb_entries)  ** (-beta)\n",
    "        obs_t, act_t, rews, obs_t1, dones = [], [], [], [], []\n",
    "\n",
    "        experiences = list()\n",
    "        for idx in idxs:\n",
    "            while idx < self.window_length + 1:\n",
    "                idx += 1\n",
    "\n",
    "            terminal0 = self.terminals[idx - 2]\n",
    "            while terminal0:\n",
    "                # Skip this transition because the environment was reset here. Select a new, random\n",
    "                # transition and use this instead. This may cause the batch to contain the same\n",
    "                # transition twice.\n",
    "                idx = sample_batch_indexes(self.window_length + 1, self.nb_entries, size=1)[0]\n",
    "                terminal0 = self.terminals[idx - 2]\n",
    "\n",
    "            assert self.window_length + 1 <= idx < self.nb_entries\n",
    "\n",
    "            #probability of sampling transition is the priority of the transition over the sum of all priorities\n",
    "            prob_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "            importance_weight = (prob_sample * self.nb_entries) ** (-beta)\n",
    "            #normalize weights according to the maximum value\n",
    "            importance_weights.append(importance_weight/max_importance_weight)\n",
    "\n",
    "            # Code for assembling stacks of observations and dealing with episode boundaries is borrowed from\n",
    "            # SequentialMemory\n",
    "            state0 = [self.observations[idx - 1]]\n",
    "            for offset in range(0, self.window_length - 1):\n",
    "                current_idx = idx - 2 - offset\n",
    "                assert current_idx >= 1\n",
    "                current_terminal = self.terminals[current_idx - 1]\n",
    "                if current_terminal and not self.ignore_episode_boundaries:\n",
    "                    # The previously handled observation was terminal, don't add the current one.\n",
    "                    # Otherwise we would leak into a different episode.\n",
    "                    break\n",
    "                state0.insert(0, self.observations[current_idx])\n",
    "            while len(state0) < self.window_length:\n",
    "                state0.insert(0, zeroed_observation(state0[0]))\n",
    "            action = self.actions[idx - 1]\n",
    "            reward = self.rewards[idx - 1]\n",
    "            terminal1 = self.terminals[idx - 1]\n",
    "            state1 = [np.copy(x) for x in state0[1:]]\n",
    "            state1.append(self.observations[idx])\n",
    "\n",
    "            assert len(state0) == self.window_length\n",
    "            assert len(state1) == len(state0)\n",
    "            experiences.append(Experience(state0=state0, action=action, reward=reward,\n",
    "                                          state1=state1, terminal1=terminal1))\n",
    "        assert len(experiences) == batch_size\n",
    "\n",
    "        # Return a tuple whre the first batch_size items are the transititions\n",
    "        # while -2 is the importance weights of those transitions and -1 is\n",
    "        # the idxs of the buffer (so that we can update priorities later)\n",
    "        return tuple(list(experiences)+ [importance_weights, idxs])\n",
    "\n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        #adjust priorities based on new TD error\n",
    "        for i, idx in enumerate(idxs):\n",
    "            assert 0 <= idx < self.limit\n",
    "            priority = priorities[i] ** self.alpha\n",
    "            self.sum_tree[idx] = priority\n",
    "            self.min_tree[idx] = priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "#         print 'priority :',priority\n",
    "    def calculate_beta(self, current_step):\n",
    "        a = float(self.end_beta - self.start_beta) / float(self.steps_annealed)\n",
    "        b = float(self.start_beta)\n",
    "        current_beta = min(self.end_beta, a * float(current_step) + b)\n",
    "        return current_beta\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PrioritizedMemory, self).get_config()\n",
    "        config['alpha'] = self.alpha\n",
    "        config['start_beta'] = self.start_beta\n",
    "        config['end_beta'] = self.end_beta\n",
    "        config['beta_steps_annealed'] = self.steps_annealed\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        \"\"\"Return number of observations\n",
    "        # Returns\n",
    "            Number of observations\n",
    "        \"\"\"\n",
    "        return len(self.observations)\n",
    "    \n",
    "#PrioritizedMemory內部的function\n",
    "def sample_batch_indexes(low, high, size): \n",
    "    \"\"\"Return a sample of (size) unique elements between low and high\n",
    "        # Argument\n",
    "            low (int): The minimum value for our samples\n",
    "            high (int): The maximum value for our samples\n",
    "            size (int): The number of samples to pick\n",
    "        # Returns\n",
    "            A list of samples of length size, with values between low and high\n",
    "        \"\"\"\n",
    "    if high - low >= size:\n",
    "        # We have enough data. Draw without replacement, that is each index is unique in the\n",
    "        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n",
    "        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n",
    "        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n",
    "        try:\n",
    "            r = xrange(low, high)\n",
    "        except NameError:\n",
    "            r = range(low, high)\n",
    "        batch_idxs = random.sample(r, size)\n",
    "    else:\n",
    "        # Not enough data. Help ourselves with sampling from the range, but the same index\n",
    "        # can occur multiple times. This is not good and should be avoided by picking a\n",
    "        # large enough warm-up phase.\n",
    "        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
    "        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
    "    assert len(batch_idxs) == size\n",
    "    return batch_idxs\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return element of buffer at specific index\n",
    "        # Argument\n",
    "            idx (int): Index wanted\n",
    "        # Returns\n",
    "            The element of buffer at given index\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        \"\"\"Append an element to the buffer\n",
    "        # Argument\n",
    "            v (object): Element to append\n",
    "        \"\"\"\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length.\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item.\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            # This should never happen.\n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "class SegmentTree(object):\n",
    "    \"\"\"\n",
    "    Abstract SegmentTree data structure used to create PrioritizedMemory.\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "\n",
    "        #powers of two have no bits in common with the previous integer\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"Capacity must be positive and a power of 2\"\n",
    "        self._capacity = capacity\n",
    "\n",
    "        #a segment tree has (2*n)-1 total nodes\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "\n",
    "        self._operation = operation\n",
    "\n",
    "        self.next_index = 0\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    \"\"\"\n",
    "    SumTree allows us to sum priorities of transitions in order to assign each a probability of being sampled.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    \"\"\"\n",
    "    In PrioritizedMemory, we normalize importance weights according to the maximum weight in the buffer.\n",
    "    This is determined by the minimum transition priority. This MinTree provides an efficient way to\n",
    "    calculate that.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "跟原本Dense唯一的不同是增加normal distribution的Noisy，使agent有機會選擇不同action，增加Exploration。\n",
    "\n",
    "Reference: https://arxiv.org/abs/1706.10295\n",
    "\"\"\"\n",
    "# class NoisyNetDense(Layer):\n",
    "#     def __init__(self,\n",
    "#                 units,\n",
    "#                 activation=None,\n",
    "#                 kernel_constraint=None,\n",
    "#                 bias_constraint=None,\n",
    "#                 kernel_regularizer=None,\n",
    "#                 bias_regularizer=None,\n",
    "#                 mu_initializer=None,\n",
    "#                 sigma_initializer=None,\n",
    "#                 **kwargs):\n",
    "\n",
    "#         super(NoisyNetDense, self).__init__(**kwargs)\n",
    "#         self.units = units\n",
    "#         self.activation = activations.get(activation)\n",
    "#         self.kernel_constraint = constraints.get(kernel_constraint) if kernel_constraint is not None else None\n",
    "#         self.bias_constraint = constriants.get(bias_constraint)if kernel_constraint is not None else None\n",
    "#         self.kernel_regularizer = regularizers.get(kernel_regularizer)if kernel_constraint is not None else None\n",
    "#         self.bias_regularizer = regularizers.get(bias_regularizer) if kernel_constraint is not None else None\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.input_dim = input_shape[-1]\n",
    "\n",
    "#         #See section 3.2 of Fortunato et al.\n",
    "#         sqr_inputs = self.input_dim**(1/2)\n",
    "#         self.sigma_initializer = initializers.Constant(value=.5/sqr_inputs)\n",
    "#         self.mu_initializer = initializers.RandomUniform(minval=(-1/sqr_inputs), maxval=(1/sqr_inputs))\n",
    "\n",
    "\n",
    "#         self.mu_weight = self.add_weight(shape=(self.input_dim, self.units),\n",
    "#                                         initializer=self.mu_initializer,\n",
    "#                                         name='mu_weights',\n",
    "#                                         constraint=self.kernel_constraint,\n",
    "#                                         regularizer=self.kernel_regularizer)\n",
    "\n",
    "#         self.sigma_weight = self.add_weight(shape=(self.input_dim, self.units),\n",
    "#                                         initializer=self.sigma_initializer,\n",
    "#                                         name='sigma_weights',\n",
    "#                                         constraint=self.kernel_constraint,\n",
    "#                                         regularizer=self.kernel_regularizer)\n",
    "\n",
    "#         self.mu_bias = self.add_weight(shape=(self.units,),\n",
    "#                                         initializer=self.mu_initializer,\n",
    "#                                         name='mu_bias',\n",
    "#                                         constraint=self.bias_constraint,\n",
    "#                                         regularizer=self.bias_regularizer)\n",
    "\n",
    "#         self.sigma_bias = self.add_weight(shape=(self.units,),\n",
    "#                                         initializer=self.sigma_initializer,\n",
    "#                                         name='sigma_bias',\n",
    "#                                         constraint=self.bias_constraint,\n",
    "#                                         regularizer=self.bias_regularizer)\n",
    "\n",
    "#         super(NoisyNetDense, self).build(input_shape=input_shape)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         #sample from noise distribution\n",
    "#         e_i = K.random_normal((self.input_dim, self.units))\n",
    "#         e_j = K.random_normal((self.units,))\n",
    "\n",
    "#         #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al.\n",
    "#         eW = K.sign(e_i)*(K.sqrt(K.abs(e_i))) * K.sign(e_j)*(K.sqrt(K.abs(e_j)))\n",
    "#         eB = K.sign(e_j)*(K.abs(e_j)**(1/2))\n",
    "\n",
    "#         #See section 3 of Fortunato et al.\n",
    "#         noise_injected_weights = K.dot(x, self.mu_weight + (self.sigma_weight * eW))\n",
    "#         noise_injected_bias = self.mu_bias + (self.sigma_bias * eB)\n",
    "#         output = K.bias_add(noise_injected_weights, noise_injected_bias)\n",
    "#         if self.activation != None:\n",
    "#             output = self.activation(output)\n",
    "#         return output\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         output_shape = list(input_shape)\n",
    "#         output_shape[-1] = self.units\n",
    "#         return tuple(output_shape)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             'units': self.units,\n",
    "#             'activation': activations.serialize(self.activation),\n",
    "#             'mu_initializer': initializers.serialize(self.mu_initializer),\n",
    "#             'sigma_initializer': initializers.serialize(self.sigma_initializer),\n",
    "#             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "#             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "#             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "#             'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "#         }\n",
    "#         base_config = super(NoisyNetDense, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "繼承DQN \n",
    "\"\"\"\n",
    "def mean_q(y_true, y_pred):\n",
    "    return K.mean(K.max(y_pred, axis=-1))\n",
    "class DQNAgent(AbstractDQNAgent):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        model__: A Keras model.\n",
    "        policy__: A Keras-rl policy that are defined in [policy](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).\n",
    "        test_policy__: A Keras-rl policy.\n",
    "        enable_double_dqn__: A boolean which enables target network as a second network proposed by van Hasselt et al. to decrease overfitting.\n",
    "        enable_dueling_dqn__: A boolean which enables dueling architecture proposed by Wang et al.\n",
    "        dueling_type__: If `enable_dueling_dqn` is set to `True`, a type of dueling architecture must be chosen which calculate Q(s,a) from V(s) and A(s,a) differently. Note that `avg` is recommanded in the [paper](https://arxiv.org/abs/1511.06581).\n",
    "            `avg`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))\n",
    "            `max`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))\n",
    "            `naive`: Q(s,a;theta) = V(s;theta) + A(s,a;theta)\n",
    "        nb_actions__: The total number of actions the agent can take. Dependent on the environment.\n",
    "        processor__: A Keras-rl processor. An intermediary between the environment and the agent. Resizes the input, clips rewards etc. Similar to gym env wrappers.\n",
    "        nb_steps_warmup__: An integer number of random steps to take before learning begins. This puts experience into the memory.\n",
    "        gamma__: The discount factor of future rewards in the Q function.\n",
    "        target_model_update__: How often to update the target model. Longer intervals stabilize training.\n",
    "        train_interval__: The integer number of steps between each learning process.\n",
    "        delta_clip__: A component of the huber loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, policy=None, test_policy=None, enable_double_dqn=True, enable_dueling_network=False,\n",
    "                 dueling_type='avg', *args, **kwargs):\n",
    "        super(DQNAgent, self).__init__(*args, **kwargs)\n",
    "#         self.custom_model_objects={\"NoisyNetDense\":NoisyNetDense}\n",
    "        self.history_q_values = []\n",
    "        # Validate (important) input.\n",
    "        if hasattr(model.output, '__len__') and len(model.output) > 1:\n",
    "            raise ValueError('Model \"{}\" has more than one output. DQN expects a model that has a single output.'.format(model))\n",
    "        if model.output._keras_shape != (None, self.nb_actions):\n",
    "            raise ValueError('Model output \"{}\" has invalid shape. DQN expects a model that has one dimension for each action, in this case {}.'.format(model.output, self.nb_actions))\n",
    "\n",
    "        # Parameters.\n",
    "        self.enable_double_dqn = enable_double_dqn\n",
    "        self.enable_dueling_network = enable_dueling_network\n",
    "        self.dueling_type = dueling_type\n",
    "        if self.enable_dueling_network:\n",
    "            # get the second last layer of the model, abandon the last layer\n",
    "            layer = model.layers[-2]\n",
    "            nb_action = model.output._keras_shape[-1]\n",
    "            # layer y has a shape (nb_action+1,)\n",
    "            # y[:,0] represents V(s;theta)\n",
    "            # y[:,1:] represents A(s,a;theta)\n",
    "            y = Dense(nb_action + 1, activation='linear')(layer.output)\n",
    "#             if type(layer) == NoisyNetDense:\n",
    "#                 y = NoisyNetDense(nb_action + 1, activation='linear')(layer.output)\n",
    "            # caculate the Q(s,a;theta)\n",
    "            # dueling_type == 'avg'\n",
    "            # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))\n",
    "            # dueling_type == 'max'\n",
    "            # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))\n",
    "            # dueling_type == 'naive'\n",
    "            # Q(s,a;theta) = V(s;theta) + A(s,a;theta)\n",
    "            if self.dueling_type == 'avg':\n",
    "                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.mean(a[:, 1:], keepdims=True), output_shape=(nb_action,))(y)\n",
    "            elif self.dueling_type == 'max':\n",
    "                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.max(a[:, 1:], keepdims=True), output_shape=(nb_action,))(y)\n",
    "            elif self.dueling_type == 'naive':\n",
    "                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:], output_shape=(nb_action,))(y)\n",
    "            else:\n",
    "                assert False, \"dueling_type must be one of {'avg','max','naive'}\"\n",
    "\n",
    "            model = Model(inputs=model.input, outputs=outputlayer)\n",
    "\n",
    "        # Related objects.\n",
    "        self.model = model\n",
    "        if policy is None:\n",
    "            policy = EpsGreedyQPolicy()\n",
    "        if test_policy is None:\n",
    "            test_policy = GreedyQPolicy()\n",
    "        self.policy = policy\n",
    "        self.test_policy = test_policy\n",
    "        self.wait = 0\n",
    "        self.monitor_max_value = 0\n",
    "        self.reset_states()\n",
    "\n",
    "        #flag for changes to algorithm that come from dealing with importance sampling weights and priorities\n",
    "        self.prioritized = True if type(self.memory) == PrioritizedMemory else False\n",
    "\n",
    "    def fit(self, env, check_env, file_nm, patience, check_interval, nb_steps, action_repetition=1, \\\n",
    "            callbacks=None, verbose=1,\\\n",
    "            visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000,\n",
    "            nb_max_episode_steps=None):\n",
    "        \"\"\"Trains the agent on the given environment.\n",
    "\n",
    "        # Arguments\n",
    "            env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
    "            nb_steps (integer): Number of training steps to be performed.\n",
    "            action_repetition (integer): Number of times the agent repeats the same action without\n",
    "                observing the environment again. Setting this to a value > 1 can be useful\n",
    "                if a single action only has a very small effect on the environment.\n",
    "            callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
    "                List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
    "            verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
    "            visualize (boolean): If `True`, the environment is visualized during training. However,\n",
    "                this is likely going to slow down training significantly and is thus intended to be\n",
    "                a debugging instrument.\n",
    "            nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
    "                of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
    "                the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
    "                at the beginning of each episode.\n",
    "            start_step_policy (`lambda observation: action`): The policy\n",
    "                to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
    "            log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
    "            nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
    "                automatically resetting the environment. Set to `None` if each episode should run\n",
    "                (potentially indefinitely) until the environment signals a terminal state.\n",
    "\n",
    "        # Returns\n",
    "            A `keras.callbacks.History` instance that recorded the entire training process.\n",
    "        \"\"\"\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError('Your tried to fit your agent but it hasn\\'t been compiled yet. Please call `compile()` before `fit()`.')\n",
    "        if action_repetition < 1:\n",
    "            raise ValueError('action_repetition must be >= 1, is {}'.format(action_repetition))\n",
    "        self.training = True\n",
    "        self.file_nm = file_nm\n",
    "        self._patience = patience\n",
    "\n",
    "        callbacks = [] if not callbacks else callbacks[:]\n",
    "        \n",
    "#         if verbose == 1:\n",
    "#             callbacks += [TrainIntervalLogger(interval=log_interval)]\n",
    "#         elif verbose > 1:\n",
    "#             callbacks += [TrainEpisodeLogger()]   \n",
    "        if visualize:\n",
    "            callbacks += [Visualizer()]\n",
    "        history = History()\n",
    "        callbacks += [history]\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        if hasattr(callbacks, 'set_model'):\n",
    "            callbacks.set_model(self)\n",
    "        else:\n",
    "            callbacks._set_model(self)\n",
    "        callbacks._set_env(env)\n",
    "        params = {\n",
    "            'nb_steps': nb_steps,\n",
    "        }\n",
    "        if hasattr(callbacks, 'set_params'):\n",
    "            callbacks.set_params(params)\n",
    "        else:\n",
    "            callbacks._set_params(params)\n",
    "        self._on_train_begin()\n",
    "        callbacks.on_train_begin()\n",
    "\n",
    "        episode = np.int16(0)\n",
    "        self.step = np.int16(0)\n",
    "        observation = None\n",
    "        episode_reward = None\n",
    "        episode_step = None\n",
    "        did_abort = False\n",
    "        try:\n",
    "            while self.step < nb_steps:\n",
    "                if observation is None:  # start of a new episode\n",
    "                    callbacks.on_episode_begin(episode)\n",
    "                    episode_step = np.int16(0)\n",
    "                    episode_reward = np.float32(0)\n",
    "\n",
    "                    # Obtain the initial observation by resetting the environment.\n",
    "                    self.reset_states()\n",
    "                    observation = deepcopy(env.reset())\n",
    "                    if self.processor is not None:\n",
    "                        observation = self.processor.process_observation(observation)\n",
    "                    assert observation is not None\n",
    "\n",
    "                    # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "                    # This slightly changes the start position between games.\n",
    "                    nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(nb_max_start_steps)\n",
    "                    for _ in range(nb_random_start_steps):\n",
    "                        if start_step_policy is None:\n",
    "                            action = env.action_space.sample()\n",
    "                        else:\n",
    "                            action = start_step_policy(observation)\n",
    "                        if self.processor is not None:\n",
    "                            action = self.processor.process_action(action)\n",
    "                        callbacks.on_action_begin(action)\n",
    "                        observation, reward, done, info = env.step(action)\n",
    "                        observation = deepcopy(observation)\n",
    "                        if self.processor is not None:\n",
    "                            observation, reward, done, info = self.processor.process_step(observation, reward, done, info)\n",
    "                        callbacks.on_action_end(action)\n",
    "                        if done:\n",
    "                            warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(nb_random_start_steps))\n",
    "                            observation = deepcopy(env.reset())\n",
    "                            if self.processor is not None:\n",
    "                                observation = self.processor.process_observation(observation)\n",
    "                            break\n",
    "\n",
    "                # At this point, we expect to be fully initialized.\n",
    "                assert episode_reward is not None\n",
    "                assert episode_step is not None\n",
    "                assert observation is not None\n",
    "\n",
    "                # Run a single step.\n",
    "                callbacks.on_step_begin(episode_step)\n",
    "                # This is were all of the work happens. We first perceive and compute the action\n",
    "                # (forward step) and then use the reward to improve (backward step).\n",
    "                action = self.forward(observation)\n",
    "                if self.processor is not None:\n",
    "                    action = self.processor.process_action(action)\n",
    "                reward = np.float32(0)\n",
    "                accumulated_info = {}\n",
    "                done = False\n",
    "                for _ in range(action_repetition):\n",
    "                    callbacks.on_action_begin(action)\n",
    "                    observation, r, done, info = env.step(action)\n",
    "                    observation = deepcopy(observation)\n",
    "                    if self.processor is not None:\n",
    "                        observation, r, done, info = self.processor.process_step(observation, r, done, info)\n",
    "                    for key, value in info.items():\n",
    "                        if not np.isreal(value):\n",
    "                            continue\n",
    "                        if key not in accumulated_info:\n",
    "                            accumulated_info[key] = np.zeros_like(value)\n",
    "                        accumulated_info[key] += value\n",
    "                    callbacks.on_action_end(action)\n",
    "                    reward += r\n",
    "                    if done:\n",
    "                        break\n",
    "                if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n",
    "                    # Force a terminal state.\n",
    "                    done = True\n",
    "                metrics = self.backward(reward, terminal=done)\n",
    "                episode_reward += reward\n",
    "\n",
    "                step_logs = {\n",
    "                    'action': action,\n",
    "                    'observation': observation,\n",
    "                    'reward': reward,\n",
    "                    'metrics': metrics,\n",
    "                    'episode': episode,\n",
    "                    'info': accumulated_info,\n",
    "                }\n",
    "                callbacks.on_step_end(episode_step, step_logs)\n",
    "                episode_step += 1\n",
    "                self.step += 1\n",
    "\n",
    "                if done:\n",
    "                    # We are in a terminal state but the agent hasn't yet seen it. We therefore\n",
    "                    # perform one more forward-backward call and simply ignore the action before\n",
    "                    # resetting the environment. We need to pass in `terminal=False` here since\n",
    "                    # the *next* state, that is the state of the newly reset environment, is\n",
    "                    # always non-terminal by convention.\n",
    "                    self.forward(observation)\n",
    "                    self.backward(0., terminal=False)\n",
    "\n",
    "                    # This episode is finished, report and reset.\n",
    "                    episode_logs = {\n",
    "                        'episode_reward': episode_reward,\n",
    "                        'nb_episode_steps': episode_step,\n",
    "                        'nb_steps': self.step,\n",
    "                        'accumulate_return': np.float32(env._params['accumulate_return']),\n",
    "                        'benchmark_accumulate_return': np.float32(env._params['benchmark_accumulate_return']),\n",
    "                        'index_nm': env._params['price_col'],\n",
    "                    }\n",
    "                    callbacks.on_episode_end(episode, episode_logs)\n",
    "                    episode += 1                    \n",
    "                    if (episode % check_interval ==0) and (episode>=4000):\n",
    "                        _, monitor_list = self.test(env=check_env,nb_episodes=len(check_env._params['data_list']), visualize=False)\n",
    "                        did_abort = self.check_model(file_nm = self.file_nm, patience = self._patience, monitor_list = monitor_list)\n",
    "#                     print(did_abort)\n",
    "                    if did_abort:\n",
    "                        nb_steps = self.step\n",
    "                    observation = None\n",
    "                    episode_step = None\n",
    "                    episode_reward = None\n",
    "        except KeyboardInterrupt:\n",
    "            # We catch keyboard interrupts here so that training can be be safely aborted.\n",
    "            # This is so common that we've built this right into this function, which ensures that\n",
    "            # the `on_train_end` method is properly called.\n",
    "            did_abort = True\n",
    "        callbacks.on_train_end(logs={'did_abort': did_abort})\n",
    "\n",
    "        self._on_train_end()\n",
    "\n",
    "        return history\n",
    "    def test(self, env, filepath='', nb_episodes=1, action_repetition=1, callbacks=None, visualize=True,\n",
    "             nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1):\n",
    "        \"\"\"Callback that is called before training begins.\n",
    "\n",
    "        # Arguments\n",
    "            env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
    "            nb_episodes (integer): Number of episodes to perform.\n",
    "            action_repetition (integer): Number of times the agent repeats the same action without\n",
    "                observing the environment again. Setting this to a value > 1 can be useful\n",
    "                if a single action only has a very small effect on the environment.\n",
    "            callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
    "                List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
    "            verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
    "            visualize (boolean): If `True`, the environment is visualized during training. However,\n",
    "                this is likely going to slow down training significantly and is thus intended to be\n",
    "                a debugging instrument.\n",
    "            nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n",
    "                of each episode using `start_step_policy`. Notice that this is an upper limit since\n",
    "                the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
    "                at the beginning of each episode.\n",
    "            start_step_policy (`lambda observation: action`): The policy\n",
    "                to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
    "            log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
    "            nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n",
    "                automatically resetting the environment. Set to `None` if each episode should run\n",
    "                (potentially indefinitely) until the environment signals a terminal state.\n",
    "\n",
    "        # Returns\n",
    "            A `keras.callbacks.History` instance that recorded the entire training process.\n",
    "        \"\"\"\n",
    "        print('test!')\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError('Your tried to test your agent but it hasn\\'t been compiled yet. Please call `compile()` before `test()`.')\n",
    "        if action_repetition < 1:\n",
    "            raise ValueError('action_repetition must be >= 1, is {}'.format(action_repetition))\n",
    "        self.history_q_values = []\n",
    "        self.training = False\n",
    "        self.step = 0\n",
    "        self.monitor_list = []\n",
    "        callbacks = [] if not callbacks else callbacks[:]\n",
    "        if filepath:\n",
    "            callbacks += [ToCsvLogger(filepath=filepath)]\n",
    "        \n",
    "        if verbose >= 1:\n",
    "            callbacks += [TestLogger()]\n",
    "#             callbacks += [Earlystop()]\n",
    "        if visualize:\n",
    "            callbacks += [Visualizer()]\n",
    "        history = History()\n",
    "        callbacks += [history]\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        if hasattr(callbacks, 'set_model'):\n",
    "            callbacks.set_model(self)\n",
    "        else:\n",
    "            callbacks._set_model(self)\n",
    "        callbacks._set_env(env)\n",
    "        params = {\n",
    "            'nb_episodes': nb_episodes,\n",
    "        }\n",
    "        if hasattr(callbacks, 'set_params'):\n",
    "            callbacks.set_params(params)\n",
    "        else:\n",
    "            callbacks._set_params(params)\n",
    "\n",
    "        self._on_test_begin()\n",
    "        callbacks.on_train_begin()\n",
    "        for episode in range(nb_episodes):\n",
    "            callbacks.on_episode_begin(episode)\n",
    "            episode_reward = 0.\n",
    "            episode_step = 0\n",
    "\n",
    "            # Obtain the initial observation by resetting the environment.\n",
    "            self.reset_states()\n",
    "            observation = deepcopy(env.reset())\n",
    "            if self.processor is not None:\n",
    "                observation = self.processor.process_observation(observation)\n",
    "            assert observation is not None\n",
    "            # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "            # This slightly changes the start position between games.\n",
    "            nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(nb_max_start_steps)\n",
    "            for _ in range(nb_random_start_steps):\n",
    "                if start_step_policy is None:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = start_step_policy(observation)\n",
    "                if self.processor is not None:\n",
    "                    action = self.processor.process_action(action)\n",
    "                callbacks.on_action_begin(action)\n",
    "                observation, r, done, info = env.step(action)\n",
    "                observation = deepcopy(observation)\n",
    "                if self.processor is not None:\n",
    "                    observation, r, done, info = self.processor.process_step(observation, r, done, info)\n",
    "                callbacks.on_action_end(action)\n",
    "                if done:\n",
    "                    warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(nb_random_start_steps))\n",
    "                    observation = deepcopy(env.reset())\n",
    "                    if self.processor is not None:\n",
    "                        observation = self.processor.process_observation(observation)\n",
    "                    break\n",
    "\n",
    "            # Run the episode until we're done.\n",
    "            done = False\n",
    "            while not done:\n",
    "                callbacks.on_step_begin(episode_step)\n",
    "\n",
    "                action = self.forward(observation)\n",
    "                if self.processor is not None:\n",
    "                    action = self.processor.process_action(action)\n",
    "                reward = 0.\n",
    "                accumulated_info = {}\n",
    "                for _ in range(action_repetition):\n",
    "                    callbacks.on_action_begin(action)\n",
    "                    observation, r, d, info = env.step(action)\n",
    "                    observation = deepcopy(observation)\n",
    "                    if self.processor is not None:\n",
    "                        observation, r, d, info = self.processor.process_step(observation, r, d, info)\n",
    "                    callbacks.on_action_end(action)\n",
    "                    reward += r\n",
    "                    for key, value in info.items():\n",
    "                        if not np.isreal(value):\n",
    "                            continue\n",
    "                        if key not in accumulated_info:\n",
    "                            accumulated_info[key] = np.zeros_like(value)\n",
    "                        accumulated_info[key] += value\n",
    "                    if d:\n",
    "                        done = True\n",
    "                        break\n",
    "                if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n",
    "                    done = True\n",
    "#                 self.backward(reward, terminal=done)\n",
    "                episode_reward += reward\n",
    "\n",
    "                step_logs = {\n",
    "                    'action': action,\n",
    "                    'observation': observation,\n",
    "                    'reward': reward,\n",
    "                    'episode': episode,\n",
    "                    'info': accumulated_info,\n",
    "                }\n",
    "                callbacks.on_step_end(episode_step, step_logs)\n",
    "                episode_step += 1\n",
    "                self.step += 1\n",
    "\n",
    "            # We are in a terminal state but the agent hasn't yet seen it. We therefore\n",
    "            # perform one more forward-backward call and simply ignore the action before\n",
    "            # resetting the environment. We need to pass in `terminal=False` here since\n",
    "            # the *next* state, that is the state of the newly reset environment, is\n",
    "            # always non-terminal by convention.\n",
    "            self.forward(observation)\n",
    "#             self.backward(0., terminal=False)\n",
    "\n",
    "            # Report end of episode.\n",
    "            episode_logs = {\n",
    "                'episode_reward': episode_reward,\n",
    "                'nb_steps': episode_step,\n",
    "                'index_nm': env._params['price_col'],\n",
    "                'test_length' : len(env._params['data_list']),\n",
    "                'accumulate_return': np.float32(env._params['accumulate_return']),\n",
    "                'benchmark_accumulate_return': np.float32(env._params['benchmark_accumulate_return']),\n",
    "                \n",
    "            }\n",
    "            self.monitor_list.append(episode_logs['accumulate_return'])\n",
    "            callbacks.on_episode_end(episode, episode_logs)\n",
    "            \n",
    "        callbacks.on_train_end()\n",
    "        self._on_test_end()\n",
    "        self.training = True\n",
    "\n",
    "        return history , self.monitor_list\n",
    "                                 \n",
    "    def get_config(self):\n",
    "        config = super(DQNAgent, self).get_config()\n",
    "        config['enable_double_dqn'] = self.enable_double_dqn\n",
    "        config['dueling_type'] = self.dueling_type\n",
    "        config['enable_dueling_network'] = self.enable_dueling_network\n",
    "        config['model'] = get_object_config(self.model)\n",
    "        config['policy'] = get_object_config(self.policy)\n",
    "        config['test_policy'] = get_object_config(self.test_policy)\n",
    "        if self.compiled:\n",
    "            config['target_model'] = get_object_config(self.target_model)\n",
    "        return config\n",
    "\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        metrics += [mean_q]  # register default metrics\n",
    "\n",
    "        # We never train the target model, hence we can set the optimizer and loss arbitrarily.\n",
    "        self.target_model = clone_model(self.model, self.custom_model_objects)\n",
    "        self.target_model.compile(optimizer='sgd', loss='mse')\n",
    "        self.model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "        # Compile model.\n",
    "        if self.target_model_update < 1.:\n",
    "            # We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\n",
    "            updates = get_soft_target_model_updates(self.target_model, self.model, self.target_model_update)\n",
    "            optimizer = AdditionalUpdatesOptimizer(optimizer, updates)\n",
    "\n",
    "        def clipped_masked_error(args):\n",
    "            y_true, y_pred, importance_weights, mask = args\n",
    "            loss = huber_loss(y_true, y_pred, self.delta_clip)\n",
    "            loss *= mask  # apply element-wise mask\n",
    "            #adjust updates by importance weights. Note that importance weights are just 1.0\n",
    "            #(and have no effect) if not using a prioritized memory\n",
    "            return K.sum(loss * importance_weights, axis=-1)\n",
    "\n",
    "        # Create trainable model. The problem is that we need to mask the output since we only\n",
    "        # ever want to update the Q values for a certain action. The way we achieve this is by\n",
    "        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n",
    "        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n",
    "        y_pred = self.model.output\n",
    "        y_true = Input(name='y_true', shape=(self.nb_actions,))\n",
    "        mask = Input(name='mask', shape=(self.nb_actions,))\n",
    "        importance_weights = Input(name='importance_weights',shape=(self.nb_actions,))\n",
    "        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_true, y_pred, importance_weights, mask])\n",
    "        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n",
    "        trainable_model = Model(inputs=ins + [y_true, importance_weights, mask], outputs=[loss_out, y_pred])\n",
    "        assert len(trainable_model.output_names) == 2\n",
    "        combined_metrics = {trainable_model.output_names[1]: metrics}\n",
    "        losses = [\n",
    "            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n",
    "            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n",
    "        ]\n",
    "        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n",
    "        self.trainable_model = trainable_model\n",
    "\n",
    "        self.compiled = True\n",
    "    \n",
    "    def check_model(self,  file_nm, patience,  monitor_list):\n",
    "        stop_training = False\n",
    "        self.monitor_value_now = np.mean(monitor_list)\n",
    "        filepath = file_nm.format(self.monitor_value_now)\n",
    "        print('max:',self.monitor_max_value)\n",
    "        print('now:',self.monitor_value_now)\n",
    "        if filepath :\n",
    "            print('check!')\n",
    "            if self.monitor_value_now > self.monitor_max_value :\n",
    "                print('accumulate_return improved from %0.5f to %0.5f, saving model to %s'\n",
    "                  % (self.monitor_max_value,self.monitor_value_now, filepath))\n",
    "                self.model.save_weights(filepath, overwrite=True) \n",
    "                self.monitor_max_value = self.monitor_value_now \n",
    "                self.wait = 0\n",
    "                self.monitor_list = []\n",
    "            else:\n",
    "                self.wait += 1  \n",
    "                if self.wait>=patience:\n",
    "                    print('accumulate_return did not improve and stop training')\n",
    "                    stop_training = True\n",
    "\n",
    "        print('wait:',self.wait)\n",
    "        print('patience:',patience)\n",
    "        return stop_training\n",
    "                \n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "        self.update_target_model_hard()\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=False):\n",
    "        self.model.save_weights(filepath, overwrite=overwrite)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.recent_action = None\n",
    "        self.recent_observation = None\n",
    "        if self.compiled:\n",
    "            self.model.reset_states()\n",
    "            self.target_model.reset_states()\n",
    "\n",
    "    def update_target_model_hard(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def forward(self, observation):\n",
    "        # Select an action.\n",
    "        state = self.memory.get_recent_state(observation)\n",
    "        q_values = self.compute_q_values(state)\n",
    "#        print(q_values)\n",
    "        self.history_q_values.append(q_values)\n",
    "        if self.training:\n",
    "            action = self.policy.select_action(q_values=q_values)\n",
    "        else:\n",
    "            action = self.test_policy.select_action(q_values=q_values)\n",
    "\n",
    "        # Book-keeping.\n",
    "        self.recent_observation = observation\n",
    "        self.recent_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "    def backward(self, reward, terminal):\n",
    "        # Store most recent experience in memory.\n",
    "        if self.step % self.memory_interval == 0:\n",
    "            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n",
    "                               training=self.training)\n",
    "\n",
    "        metrics = [np.nan for _ in self.metrics_names]\n",
    "        if not self.training:\n",
    "            # We're done here. No need to update the experience memory since we only use the working\n",
    "            # memory to obtain the state over the most recent observations.\n",
    "            return metrics\n",
    "\n",
    "        # Train the network on a single stochastic batch.\n",
    "        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n",
    "\n",
    "            if self.prioritized:\n",
    "                # Calculations for current beta value based on a linear schedule.\n",
    "                current_beta = self.memory.calculate_beta(self.step)\n",
    "                # Sample from the memory.\n",
    "                experiences = self.memory.sample(self.batch_size, current_beta)\n",
    "            else:\n",
    "                #SequentialMemory\n",
    "                experiences = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Start by extracting the necessary parameters (we use a vectorized implementation).\n",
    "            state0_batch = []\n",
    "            reward_batch = []\n",
    "            action_batch = []\n",
    "            terminal1_batch = []\n",
    "            state1_batch = []\n",
    "            importance_weights = []\n",
    "            # We will be updating the idxs of the priority trees with new priorities\n",
    "            pr_idxs = []\n",
    "\n",
    "            if self.prioritized:\n",
    "                for e in experiences[:-2]: # Prioritized Replay returns Experience tuple + weights and idxs.\n",
    "                    state0_batch.append(e.state0)\n",
    "                    state1_batch.append(e.state1)\n",
    "                    reward_batch.append(e.reward)\n",
    "                    action_batch.append(e.action)\n",
    "                    terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "                importance_weights = experiences[-2]\n",
    "                pr_idxs = experiences[-1]\n",
    "            else: #SequentialMemory\n",
    "                for e in experiences:\n",
    "                    state0_batch.append(e.state0)\n",
    "                    state1_batch.append(e.state1)\n",
    "                    reward_batch.append(e.reward)\n",
    "                    action_batch.append(e.action)\n",
    "                    terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "            # Prepare and validate parameters.\n",
    "            state0_batch = self.process_state_batch(state0_batch)\n",
    "            state1_batch = self.process_state_batch(state1_batch)\n",
    "            terminal1_batch = np.array(terminal1_batch)\n",
    "            reward_batch = np.array(reward_batch)\n",
    "            assert reward_batch.shape == (self.batch_size,)\n",
    "            assert terminal1_batch.shape == reward_batch.shape\n",
    "            assert len(action_batch) == len(reward_batch)\n",
    "\n",
    "            # Compute Q values for mini-batch update.\n",
    "            if self.enable_double_dqn:\n",
    "                # According to the paper \"Deep Reinforcement Learning with Double Q-learning\"\n",
    "                # (van Hasselt et al., 2015), in Double DQN, the online network predicts the actions\n",
    "                # while the target network is used to estimate the Q value.\n",
    "                q_values = self.model.predict_on_batch(state1_batch)\n",
    "                assert q_values.shape == (self.batch_size, self.nb_actions)\n",
    "                actions = np.argmax(q_values, axis=1)\n",
    "                assert actions.shape == (self.batch_size,)\n",
    "\n",
    "                # Now, estimate Q values using the target network but select the values with the\n",
    "                # highest Q value wrt to the online model (as computed above).\n",
    "                target_q_values = self.target_model.predict_on_batch(state1_batch)\n",
    "                assert target_q_values.shape == (self.batch_size, self.nb_actions)\n",
    "                q_batch = target_q_values[range(self.batch_size), actions]\n",
    "            else:\n",
    "                # Compute the q_values given state1, and extract the maximum for each sample in the batch.\n",
    "                # We perform this prediction on the target_model instead of the model for reasons\n",
    "                # outlined in Mnih (2015). In short: it makes the algorithm more stable.\n",
    "                target_q_values = self.target_model.predict_on_batch(state1_batch)\n",
    "                assert target_q_values.shape == (self.batch_size, self.nb_actions)\n",
    "                q_batch = np.max(target_q_values, axis=1).flatten()\n",
    "            assert q_batch.shape == (self.batch_size,)\n",
    "\n",
    "            targets = np.zeros((self.batch_size, self.nb_actions))\n",
    "            dummy_targets = np.zeros((self.batch_size,))\n",
    "            masks = np.zeros((self.batch_size, self.nb_actions))\n",
    "\n",
    "            # Compute r_t + gamma * max_a Q(s_t+1, a) and update the target targets accordingly,\n",
    "            # but only for the affected output units (as given by action_batch).\n",
    "            discounted_reward_batch = self.gamma * q_batch\n",
    "            # Set discounted reward to zero for all states that were terminal.\n",
    "            discounted_reward_batch *= terminal1_batch\n",
    "            assert discounted_reward_batch.shape == reward_batch.shape\n",
    "            #Putting together the multi-step target\n",
    "            Rs = reward_batch + discounted_reward_batch\n",
    "\n",
    "            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n",
    "                target[action] = R  # update action with estimated accumulated reward\n",
    "                dummy_targets[idx] = R\n",
    "                mask[action] = 1.  # enable loss for this specific action\n",
    "            targets = np.array(targets).astype('float32')\n",
    "            masks = np.array(masks).astype('float32')\n",
    "\n",
    "            if not self.prioritized:\n",
    "                importance_weights = [1. for _ in range(self.batch_size)]\n",
    "            #Make importance_weights the same shape as the other tensors that are passed into the trainable model\n",
    "            assert len(importance_weights) == self.batch_size\n",
    "            importance_weights = np.array(importance_weights)\n",
    "            importance_weights = np.vstack([importance_weights]*self.nb_actions)\n",
    "            importance_weights = np.reshape(importance_weights, (self.batch_size, self.nb_actions))\n",
    "            # Perform a single update on the entire batch. We use a dummy target since\n",
    "            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n",
    "            # it is still useful to know the actual target to compute metrics properly.\n",
    "            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n",
    "            metrics = self.trainable_model.train_on_batch(ins + [targets, importance_weights, masks], [dummy_targets, targets])\n",
    "\n",
    "            if self.prioritized:\n",
    "                assert len(pr_idxs) == self.batch_size\n",
    "                #Calculate new priorities.\n",
    "                y_true = targets\n",
    "                y_pred = self.model.predict_on_batch(ins)\n",
    "                #Proportional method. Priorities are the abs TD error with a small positive constant to keep them from being 0.\n",
    "                new_priorities = (abs(np.sum(y_true - y_pred, axis=-1))) + 1e-5\n",
    "                assert len(new_priorities) == self.batch_size\n",
    "                #update priorities\n",
    "                self.memory.update_priorities(pr_idxs, new_priorities)\n",
    "\n",
    "            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n",
    "            metrics += self.policy.metrics\n",
    "            if self.processor is not None:\n",
    "                metrics += self.processor.metrics\n",
    "\n",
    "        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n",
    "            self.update_target_model_hard()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.model.layers[:]\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        # Throw away individual losses and replace output name since this is hidden from the user.\n",
    "        assert len(self.trainable_model.output_names) == 2\n",
    "        dummy_output_name = self.trainable_model.output_names[1]\n",
    "        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n",
    "        model_metrics = [name.replace(dummy_output_name + '_', '') for name in model_metrics]\n",
    "\n",
    "        names = model_metrics + self.policy.metrics_names[:]\n",
    "        if self.processor is not None:\n",
    "            names += self.processor.metrics_names[:]\n",
    "        return names\n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self.__policy\n",
    "\n",
    "    @policy.setter\n",
    "    def policy(self, policy):\n",
    "        self.__policy = policy\n",
    "        self.__policy._set_agent(self)\n",
    "\n",
    "    @property\n",
    "    def test_policy(self):\n",
    "        return self.__test_policy\n",
    "\n",
    "    @test_policy.setter\n",
    "    def test_policy(self, policy):\n",
    "        self.__test_policy = policy\n",
    "        self.__test_policy._set_agent(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(KerasCallback):\n",
    "    def _set_env(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        \"\"\"Called at beginning of each episode\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        \"\"\"Called at end of each episode\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_step_begin(self, step, logs={}):\n",
    "        \"\"\"Called at beginning of each step\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        \"\"\"Called at end of each step\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_action_begin(self, action, logs={}):\n",
    "        \"\"\"Called at beginning of each action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_action_end(self, action, logs={}):\n",
    "        \"\"\"Called at end of each action\"\"\"\n",
    "        pass\n",
    "    def accumulate_return_to_month(self, step, accumulate_return):\n",
    "        return (accumulate_return+1.)**(1./step) - 1. \n",
    "\n",
    "\n",
    "class CallbackList(KerasCallbackList):\n",
    "    def _set_env(self, env):\n",
    "        \"\"\" Set environment for each callback in callbackList \"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            if callable(getattr(callback, '_set_env', None)):\n",
    "                callback._set_env(env)\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        \"\"\" Called at beginning of each episode for each callback in callbackList\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            # Check if callback supports the more appropriate `on_episode_begin` callback.\n",
    "            # If not, fall back to `on_epoch_begin` to be compatible with built-in Keras callbacks.\n",
    "            if callable(getattr(callback, 'on_episode_begin', None)):\n",
    "                callback.on_episode_begin(episode, logs=logs)\n",
    "            else:\n",
    "                callback.on_epoch_begin(episode, logs=logs)\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        \"\"\" Called at end of each episode for each callback in callbackList\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            # Check if callback supports the more appropriate `on_episode_end` callback.\n",
    "            # If not, fall back to `on_epoch_end` to be compatible with built-in Keras callbacks.\n",
    "            if callable(getattr(callback, 'on_episode_end', None)):\n",
    "                callback.on_episode_end(episode, logs=logs)\n",
    "            else:\n",
    "                callback.on_epoch_end(episode, logs=logs)\n",
    "\n",
    "    def on_step_begin(self, step, logs={}):\n",
    "        \"\"\" Called at beginning of each step for each callback in callbackList\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            # Check if callback supports the more appropriate `on_step_begin` callback.\n",
    "            # If not, fall back to `on_batch_begin` to be compatible with built-in Keras callbacks.\n",
    "            if callable(getattr(callback, 'on_step_begin', None)):\n",
    "                callback.on_step_begin(step, logs=logs)\n",
    "            else:\n",
    "                callback.on_batch_begin(step, logs=logs)\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        \"\"\" Called at end of each step for each callback in callbackList\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            # Check if callback supports the more appropriate `on_step_end` callback.\n",
    "            # If not, fall back to `on_batch_end` to be compatible with built-in Keras callbacks.\n",
    "            if callable(getattr(callback, 'on_step_end', None)):\n",
    "                callback.on_step_end(step, logs=logs)\n",
    "            else:\n",
    "                callback.on_batch_end(step, logs=logs)\n",
    "\n",
    "    def on_action_begin(self, action, logs={}):\n",
    "        \"\"\" Called at beginning of each action for each callback in callbackList\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            if callable(getattr(callback, 'on_action_begin', None)):\n",
    "                callback.on_action_begin(action, logs=logs)\n",
    "\n",
    "    def on_action_end(self, action, logs={}):\n",
    "        \"\"\" Called at end of each action for each callback in callbackList\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            if callable(getattr(callback, 'on_action_end', None)):\n",
    "                callback.on_action_end(action, logs=logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "新增Callback\n",
    "\n",
    "argument:\n",
    "    monitor:用於監控的數值，把相較於上次更好的weight存下來\n",
    "\"\"\"\n",
    "class ModelCheckByTrainAccRe(Callback): \n",
    "    def __init__(self, filepath, monitor='month_return', verbose=2,\n",
    "                 save_best_only=True, save_weights_only=True,min_save_length=3500,\n",
    "                 mode='auto'):\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.min_save_length = min_save_length\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'accumulate_return' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_episode_end(self, epoch, logs=None):\n",
    "        if epoch>self.min_save_length:\n",
    "            logs = logs or {}\n",
    "            logs['month_return'] = accumulate_return_to_month(logs['nb_episode_steps'],logs['accumulate_return'])\n",
    "            logs['benchmark_month_return'] = accumulate_return_to_month(logs['nb_episode_steps'],logs['benchmark_accumulate_return'])\n",
    "            logs['month_return_diff'] = (logs.get('month_return') - logs.get('benchmark_month_return'))\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs) \n",
    "            if self.save_best_only:\n",
    "                current_train_acc_re = logs.get(self.monitor)\n",
    "                if(current_train_acc_re > 0)&(logs.get('month_return')>0):\n",
    "                    if self.monitor_op(current_train_acc_re, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                     ' saving model to %s'\n",
    "                                      % (epoch + 1, self.monitor, self.best,\n",
    "                                         current_train_acc_re, filepath))\n",
    "                        self.best = current_train_acc_re\n",
    "\n",
    "                        if self.save_weights_only:\n",
    "                            self.model.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            self.model.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('\\nEpoch %05d: %s did not improve' %\n",
    "                                  (epoch + 1, self.monitor))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    self.model.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    self.model.save(filepath, overwrite=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-27fedf68b864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m於\u001b[0m\u001b[0mTraining\u001b[0m\u001b[0;31m時\u001b[0m\u001b[0;31m輸\u001b[0m\u001b[0;31m出\u001b[0m\u001b[0;31m各\u001b[0m\u001b[0;31m項\u001b[0m\u001b[0;31m自\u001b[0m\u001b[0;31m訂\u001b[0m\u001b[0;31m的\u001b[0m\u001b[0;31m變\u001b[0m\u001b[0;31m數\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTrainEpisodeLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Some algorithms compute multiple episodes at once since they are multi-threaded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "新增Callback\n",
    "\n",
    "於Training時輸出各項自訂的變數\n",
    "\"\"\"\n",
    "class TrainEpisodeLogger(Callback):\n",
    "    def __init__(self, filepath=None):\n",
    "        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n",
    "        # We therefore use a dictionary that is indexed by the episode to separate episodes\n",
    "        # from each other.\n",
    "        self.episode_start = {}\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "        self.metrics = {}\n",
    "        self.step = 0\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        \"\"\" Print training values at beginning of training \"\"\"\n",
    "        self.train_start = timeit.default_timer()\n",
    "        self.metrics_names = self.model.metrics_names\n",
    "        print('Training for {} steps ...'.format(self.params['nb_steps']))\n",
    "        \n",
    "    def on_train_end(self, logs):\n",
    "        \"\"\" Print training time at end of training \"\"\"\n",
    "        duration = timeit.default_timer() - self.train_start\n",
    "        print('done, took {:.3f} seconds'.format(duration))\n",
    "        if self.filepath != None:\n",
    "            with open(self.filepath,'a') as f:\n",
    "                f.write(\"WALL CLOCK TIME: \" + str(duration))\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        \"\"\" Reset environment variables at beginning of each episode \"\"\"\n",
    "        self.episode_start[episode] = timeit.default_timer()\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "        self.metrics[episode] = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        \"\"\" Compute and print training statistics of the episode when done \"\"\"\n",
    "        duration = timeit.default_timer() - self.episode_start[episode]\n",
    "        episode_steps = len(self.observations[episode])\n",
    "\n",
    "        # Format all metrics.\n",
    "        metrics = np.array(self.metrics[episode])\n",
    "        metrics_template = ''\n",
    "        metrics_variables = []\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for idx, name in enumerate(self.metrics_names):\n",
    "                if idx > 0:\n",
    "                    metrics_template += ', '\n",
    "                try:\n",
    "                    value = np.nanmean(metrics[:, idx])\n",
    "                    metrics_template += '{}: {:f}'\n",
    "                except Warning:\n",
    "                    value = '--'\n",
    "                    metrics_template += '{}: {}'\n",
    "                metrics_variables += [name, value]          \n",
    "        metrics_text = metrics_template.format(*metrics_variables)\n",
    "\n",
    "        nb_step_digits = str(int(np.ceil(np.log10(self.params['nb_steps']))) + 1)\n",
    "#         template = '{step: ' + nb_step_digits + 'd}/{nb_steps}: episode: {episode}, duration: {duration:.3f}s, episode steps: {episode_steps}, steps per second: {sps:.0f}, episode reward: {episode_reward:.3f}, mean reward: {reward_mean:.3f} [{reward_min:.3f}, {reward_max:.3f}], mean action: {action_mean:.3f} [{action_min:.3f}, {action_max:.3f}], mean observation: {obs_mean:.3f} [{obs_min:.3f}, {obs_max:.3f}], month_return: {month_return:.3f}, benchmark_month_return: {benchmark_month_return:.3f}, {metrics}'\n",
    "        template = '{step: ' + nb_step_digits + 'd}/{nb_steps}: episode: {episode}, duration: {duration:.3f}s, episode steps: {episode_steps}, steps per second: {sps:.0f}, mean reward: {reward_mean:.3f} [{reward_min:.3f}, {reward_max:.3f}], mean action: {action_mean:.3f} [{action_min:.3f}, {action_max:.3f}], mean observation: {obs_mean:.3f} [{obs_min:.3f}, {obs_max:.3f}], month_return: {month_return:.5f}, accumulate_return: {accumulate_return:.5f}, benchmark_month_return: {benchmark_month_return:.5f}, benchmark_accumulate_return: {benchmark_accumulate_return: .5f}, {metrics},nb_episode_steps: {nb_episode_steps}'\n",
    "        variables = {\n",
    "            'step': self.step,\n",
    "            'nb_steps': self.params['nb_steps'],\n",
    "            'episode': episode + 1,\n",
    "            'duration': duration,\n",
    "            'episode_steps': episode_steps,\n",
    "            'nb_episode_steps' : logs['nb_episode_steps'],\n",
    "            'sps': float(episode_steps) / duration,\n",
    "            'episode_reward': np.sum(self.rewards[episode]),\n",
    "            'reward_mean': np.mean(self.rewards[episode]),\n",
    "            'reward_min': np.min(self.rewards[episode]),\n",
    "            'reward_max': np.max(self.rewards[episode]),\n",
    "            'action_mean': np.mean(self.actions[episode]),\n",
    "            'action_min': np.min(self.actions[episode]),\n",
    "            'action_max': np.max(self.actions[episode]),\n",
    "            'obs_mean': np.mean(self.observations[episode]),\n",
    "            'obs_min': np.min(self.observations[episode]),\n",
    "            'obs_max': np.max(self.observations[episode]),\n",
    "            'accumulate_return' : logs.get('accumulate_return'),\n",
    "            'month_return' : accumulate_return_to_month(episode_steps,logs['accumulate_return']),\n",
    "            'benchmark_accumulate_return': logs.get('benchmark_accumulate_return'),\n",
    "            'benchmark_month_return' : accumulate_return_to_month(episode_steps,logs['benchmark_accumulate_return']),\n",
    "            'metrics': metrics_text,\n",
    "        }\n",
    "#         print(variables['metrics'])\n",
    "        print(template.format(**variables))\n",
    "        \n",
    "        metrics_text_list = metrics_text.replace(': ', ', ').split(', ')\n",
    "        for i in range(len(metrics_text_list))[::2]:\n",
    "            variables[metrics_text_list[i]]=metrics_text_list[i+1]\n",
    "        try:\n",
    "            if self.filepath != None:\n",
    "                if os.path.isfile(self.filepath):\n",
    "                    with open(self.filepath, 'a') as f:\n",
    "                        w = csv.DictWriter(f, variables.keys())\n",
    "                        w.writerow(variables)\n",
    "                else:\n",
    "                    with open(self.filepath, 'a') as f:\n",
    "                        w = csv.DictWriter(f, variables.keys())\n",
    "                        w.writeheader()\n",
    "                        w.writerow(variables)\n",
    "                        \n",
    "#                 with open(self.filepath, 'a') as f:\n",
    "#                     line = [str(variables[key]) + \",\" for key in sorted(variables.keys())]\n",
    "#                     f.write(str(line).replace(',','').replace('[','').replace(']','').strip())\n",
    "#                     f.write('\\n')\n",
    "\n",
    "        finally:\n",
    "        # Free up resources.\n",
    "            del self.episode_start[episode]\n",
    "            del self.observations[episode]\n",
    "            del self.rewards[episode]\n",
    "            del self.actions[episode]\n",
    "            del self.metrics[episode]\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        \"\"\" Update statistics of episode after each step \"\"\"\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "        self.metrics[episode].append(logs['metrics'])\n",
    "        self.step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "新增Callback\n",
    "\n",
    "於Training計算自定義的評估函數\n",
    "\"\"\"\n",
    "class TestLogger(Callback):\n",
    "    \"\"\" Logger Class for Test \"\"\"\n",
    "    \n",
    "    def on_train_begin(self, logs):\n",
    "        \"\"\" Print logs at beginning of training\"\"\"\n",
    "#         print('Testing for {} episodes ...'.format(self.params['nb_episodes']))\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        \"\"\" Print logs at end of each episode \"\"\"\n",
    "        template = 'Episode {episode}: reward_mean: {reward_mean:.3f}, accumulate_return: {accumulate_return:.5f}, benchmark_accumulate_return: {benchmark_accumulate_return:.5f}, nb_steps: {nb_steps}'\n",
    "        variables = {\n",
    "            'episode': episode + 1,\n",
    "            'reward_mean': float(logs['episode_reward'])/logs['nb_steps'],\n",
    "            'month_return' : accumulate_return_to_month(logs['nb_steps'],logs['accumulate_return']),\n",
    "            'accumulate_return' : logs['accumulate_return'],\n",
    "            'benchmark_accumulate_return' : logs['benchmark_accumulate_return'],\n",
    "            'benchmark_month_return' : accumulate_return_to_month(logs['nb_steps'],logs['benchmark_accumulate_return']),\n",
    "            'nb_steps' : logs['nb_steps'],\n",
    "        }\n",
    "#         print(template.format(**variables))\n",
    "class ToCsvLogger(Callback):\n",
    "    \"\"\" Logger Class for Test \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        \"\"\" Print logs at end of each episode \"\"\"\n",
    "        template = 'Episode {episode}: reward_mean: {reward_mean:.3f}, month_return: {month_return:.5f}, benchmark_month_return: {benchmark_month_return:.5f}, nb_steps: {nb_steps}'\n",
    "        variables = {\n",
    "            'episode': episode + 1,\n",
    "            'reward_mean': float(logs['episode_reward'])/logs['nb_steps'],\n",
    "            'accumulate_return' : logs['accumulate_return'],\n",
    "            'index_nm' : logs['index_nm'],\n",
    "            'benchmark_accumulate_return' : logs['benchmark_accumulate_return'],\n",
    "            'month_return' : accumulate_return_to_month(logs['nb_steps'],logs['accumulate_return']),\n",
    "            'benchmark_month_return' : accumulate_return_to_month(logs['nb_steps'],logs['benchmark_accumulate_return']),\n",
    "            'nb_steps' : logs['nb_steps'],\n",
    "        }\n",
    "        if self.filepath != None:\n",
    "            if os.path.isfile(self.filepath):\n",
    "                with open(self.filepath, 'a') as f:\n",
    "                    w = csv.DictWriter(f, variables.keys())\n",
    "                    w.writerow(variables)\n",
    "            else:\n",
    "                with open(self.filepath, 'a') as f:\n",
    "                    w = csv.DictWriter(f, variables.keys())\n",
    "                    w.writeheader()\n",
    "                    w.writerow(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "繼承Env\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tgym.core import Env, DataGenerator \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 15,\n",
    "        \"axes.labelsize\": 15,\n",
    "        \"lines.linewidth\": 1,\n",
    "        \"lines.markersize\": 8\n",
    "    }\n",
    ")\n",
    "class AbstractEnv(Env):\n",
    "    \n",
    "    _actions = {\n",
    "        'hold': 0,\n",
    "        'buy': 1,\n",
    "        'sell': 2\n",
    "    }\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self._params=params\n",
    "        assert self._params['history_length'] > 0\n",
    "        self._params['price_history'] = []\n",
    "        self._params['accumulate_return_list'] = []\n",
    "        self._params['data_index'] =-1\n",
    "        self._params['rolling_start_len'] = -1\n",
    "        \n",
    "    def reset(self):\n",
    "        self._params['cash']=self._params['cash_start']\n",
    "        self._params['total_cash']=0\n",
    "        self._params['data_index'] +=1\n",
    "        self._params['rolling_start_len'] += 1\n",
    "        self._params['data_index'] %=len(self._params['data_list'])\n",
    "        self._params['data']=self._params['data_list'][self._params['data_index']]\n",
    "        if self._params.get('StandardScaler_list'):\n",
    "            self._params['SS']=self._params['StandardScaler_list'][self._params['data_index']]\n",
    "        else:\n",
    "            SS=StandardScaler()\n",
    "            SS.fit(self._params['data'].loc[:,self._params['nn_input_col']])\n",
    "            self._params['SS']=SS\n",
    "        self._params['benchmark_position'] = 0\n",
    "        self._params['benchmark_history_trade'] = []\n",
    "        self._params['first_buy_price'] = 0\n",
    "        self._params['position'] = 0\n",
    "        self._params['sell_price'] = 0\n",
    "        self._params['buy_price'] = 0\n",
    "        self._params['action'] = self._actions['buy']\n",
    "        self._params['history_action'] = []\n",
    "        self._params['history_trade'] = []\n",
    "        self._params['accumulate_return_list'] = []\n",
    "        self._params['reward_list'] = []\n",
    "        self._params['hold_times'] = 0\n",
    "        self._params['accumulate_return'] = 0\n",
    "        self._params['total_buy_price'] = 0\n",
    "        self._params['benchmark_total_cash'] = 0\n",
    "        self._params['iteration'] = self._params['history_length']\n",
    "        self._params['trade_limit'] = len(self._params['data'])\n",
    "        self._params['hold_sell_list'] = []\n",
    "        self._params['benchmark_hold_sell_list'] = []\n",
    "        self._params['over_buy_price_list'] = []\n",
    "        self._params['benchmark_over_buy_price_list'] = []\n",
    "        self._params['enter_times'] = 0\n",
    "#         if self._params['train_lab'] :\n",
    "#             if self._params['reward_step'] :\n",
    "#                 while self._params['trade_limit'] + max(self._params['reward_step'])  >= len(self._params['data']):\n",
    "#                     self._params['iteration'] = self._params['history_length'] + random.randrange(int(len(self._params['data'])))\n",
    "#                     self._params['trade_limit'] = self._params['iteration'] + random.randrange(24,len(self._params['data']))\n",
    "#             else:\n",
    "#                 while self._params['trade_limit'] >= len(self._params['data']) :\n",
    "#                     self._params['iteration'] = self._params['history_length'] + random.randrange(int(len(self._params['data'])))\n",
    "#                     self._params['trade_limit'] = self._params['iteration'] + random.randrange(24,len(self._params['data']))\n",
    "        self._params['iteration'] += self._params['rolling_start_len']/len(self._params['data_list'])\n",
    "        if self.stop_condition()[0]:\n",
    "            self._params['rolling_start_len'] = 0\n",
    "            self._params['iteration'] = self._params['history_length']\n",
    "        self._params['trade_limit'] = len(self._params['data'])\n",
    "        self._params['history_data'] = self.gen_data ( self._params['data'], self._params['iteration'] )\n",
    "        self._params['price_history'] = self._params['history_data'][self._params['price_col']].tolist()     \n",
    "        self._params['benchmark_buy_price'] = self._params['price_history'][-1]\n",
    "        self._params['observation'] = self._get_observation()\n",
    "        return self._params['observation'][0]\n",
    "      \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action (buy/sell/hold) and computes the immediate reward.\n",
    "        Args:\n",
    "            action (numpy.array): Action to be taken, one-hot encoded.\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - observation (numpy.array): Agent's observation of the current environment.\n",
    "                - reward (float) : Amount of reward returned after previous action.\n",
    "                - done (bool): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "                - info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def stop_condition(self):\n",
    "        info = {}\n",
    "        if self._params['iteration'] >= self._params['episode_length']:\n",
    "            done = True\n",
    "            info['status'] = 'Time out.' \n",
    "        elif self._params['reward_step'] :\n",
    "            if self._params['iteration'] + max(self._params['reward_step'])  >= len(self._params['data']):\n",
    "                done = True\n",
    "                info['status'] = 'No reward data.'\n",
    "            else :\n",
    "                done = False\n",
    "        elif self._params['iteration'] >= len(self._params['data']):\n",
    "            done = True\n",
    "            info['status'] = 'No more data.' \n",
    "        elif (self._params['train_lab']) and (self._params['iteration'] >= self._params['trade_limit']):\n",
    "            done = True\n",
    "            info['status'] = 'Stop trading.' \n",
    "        else :\n",
    "            done = False\n",
    "        return done, info\n",
    "    \n",
    "    def done_true_to_do(self):\n",
    "               \n",
    "        self._params['volatility'] = self.volatility(self._params['hold_sell_list'])\n",
    "        self._params['benchmark_volatility'] = self.volatility(self._params['benchmark_hold_sell_list'])\n",
    "        self._params['lower_buy_price_num'] = np.sum([i == -1 for i in self._params['over_buy_price_list']])\n",
    "        self._params['over_buy_price_num'] = np.sum([i == 1 for i in self._params['over_buy_price_list']])\n",
    "        self._params['benchmark_lower_buy_price_num'] = np.sum([i == -1 for i in self._params['benchmark_over_buy_price_list']])\n",
    "        self._params['benchmark_over_buy_price_num'] = np.sum([i == 1 for i in self._params['benchmark_over_buy_price_list']])\n",
    "        \n",
    "        try:\n",
    "            self._params['over_buy_price_prob'] = float(self._params['over_buy_price_num'])/\\\n",
    "            (self._params['lower_buy_price_num']+self._params['over_buy_price_num'])\n",
    "        except ZeroDivisionError:\n",
    "            self._params['over_buy_price_prob'] = 0\n",
    "            \n",
    "        try:\n",
    "            self._params['benchmark_over_buy_price_prob'] = float(self._params['benchmark_over_buy_price_num'])/\\\n",
    "            (self._params['benchmark_lower_buy_price_num']+self._params['benchmark_over_buy_price_num'])     \n",
    "        except ZeroDivisionError:\n",
    "            self._params['benchmark_over_buy_price_prob'] = 0   \n",
    "        \n",
    "        try:\n",
    "            self._params['sharpe'] = self._params['accumulate_return']/self._params['volatility']\n",
    "        except ZeroDivisionError:\n",
    "            self._params['sharpe'] = 0  \n",
    "            \n",
    "        try:\n",
    "            self._params['benchmark_sharpe'] = self._params['benchmark_accumulate_return']/self._params['benchmark_volatility']\n",
    "        except ZeroDivisionError:\n",
    "            self._params['benchmark_sharpe'] = 0   \n",
    "    \n",
    "    def volatility(self,hold_sell_list):\n",
    "\n",
    "        return_list = [(hold_sell_list[i]-hold_sell_list[i-1])/float(hold_sell_list[i-1]) \\\n",
    "                        if (hold_sell_list[i-1]!=0) and (hold_sell_list[i]!=0)\\\n",
    "                        else 0  for i in range(1,len(hold_sell_list))]\n",
    "        return np.std(return_list,ddof=1)\n",
    "    \n",
    "    def gen_volatility(self,hold_sell_list):\n",
    "        hold_sell_return_list = [(hold_sell_list[i]-hold_sell_list[i-1])/float(hold_sell_list[i-1]) \\\n",
    "                                 if (hold_sell_list[i-1]!=0) and (hold_sell_list[i]!=0) else 0  \\\n",
    "                                 for i in range(1,len(hold_sell_list))]\n",
    "        return np.std(hold_sell_return_list,ddof=1)\n",
    "    \n",
    "    def gen_enter_times(self):\n",
    "        if (self._params['position'] < self._params['max_position']) and (self._params['action'] == self._actions['buy']):\n",
    "            self._params['enter_times'] += 1\n",
    "            \n",
    "    def gen_hold_sell_list(self):\n",
    "        if self._params['position'] < self._params['max_position']:\n",
    "            if (self._params['action'] == self._actions['sell']) or (self._params['action'] == self._actions['hold']):\n",
    "                self._params['hold_sell_list'].append(0)\n",
    "            else:\n",
    "                self._params['hold_sell_list'].append(self._params['price_history'][-2])\n",
    "        else:\n",
    "            self._params['hold_sell_list'].append(self._params['price_history'][-2])\n",
    "        self._params['benchmark_hold_sell_list'].append(self._params['price_history'][-2])\n",
    "                \n",
    "    def gen_over_buy_price_list(self,over_buy_price_list,buy_price):\n",
    "        #1:持有 ， -1:價錢比買價低 ， 0:沒有單位\n",
    "        if self._params['position'] == 0:\n",
    "            over_buy_price_list.append(0)\n",
    "        else:\n",
    "            if self._params['price_history'][-2] > buy_price:\n",
    "                over_buy_price_list.append(1) \n",
    "            else :\n",
    "                over_buy_price_list.append(-1) \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Concatenate all necessary elements to create the observation.\n",
    "        Returns:\n",
    "            numpy.array: observation array. \n",
    "        \"\"\"\n",
    "        if self._params['data_nor']:\n",
    "            ob_data = pd.DataFrame(self._params['SS'].transform(self._params['history_data'].loc[:,self._params['nn_input_col']]))\n",
    "        else :\n",
    "            ob_data = self._params['history_data'].loc[:,self._params['nn_input_col']]\n",
    "        if self._params['position']>0 :\n",
    "            position_lab = [1,-1]\n",
    "        else :\n",
    "            position_lab = [-1,1]\n",
    "        ob = []\n",
    "        for i in ob_data.as_matrix().T:\n",
    "            ob += i.tolist()\n",
    "        if self._params.get('trend_months'): \n",
    "            predict_months_price = []\n",
    "            for i in self._params['trend_months']:\n",
    "                #因為index從0開始的關係，所以需要-1\n",
    "                predict_months_price = predict_months_price + [self._params['data'].loc[self._params['iteration'] + i - 1,self._params['reward_col']]]\n",
    "            trend = self.trend(self._params['price_history'][-1], predict_months_price, self._params['threshold'], self._params['trend_noise'])\n",
    "            state = ob + position_lab + \\\n",
    "            [self._params['history_data'].tail(1).loc[:,i].values[0] for i in self._params['no_time_var_nm']]  + trend\n",
    "\n",
    "        else :\n",
    "            state = ob + position_lab + \\\n",
    "            [self._params['history_data'].tail(1).loc[:,i].values[0] for i in self._params['no_time_var_nm']]\n",
    "        future_months_price = []\n",
    "        if self._params.get('reward_step') :\n",
    "            for i in self._params['reward_step']:\n",
    "                future_months_price = future_months_price + [self._params['data'].loc[self._params['iteration'] + i - 1, self._params['reward_col']]]\n",
    "        else :\n",
    "            future_months_price = []\n",
    "        return [state,future_months_price]    \n",
    "#     def _get_observation(self):\n",
    "#         \"\"\"Concatenate all necessary elements to create the observation.\n",
    "#         Returns:\n",
    "#             numpy.array: observation array. \n",
    "#         \"\"\"\n",
    "#         if self._params['data_nor']:\n",
    "#             ob_data = pd.DataFrame(self._params['SS'].transform(self._params['history_data'].loc[:,self._params['nn_input_col']]))\n",
    "#         else :\n",
    "#             ob_data = self._params['history_data'].loc[:,self._params['nn_input_col']]\n",
    "#         if self._params['position']>0 :\n",
    "#             position_lab = 1\n",
    "#         else :\n",
    "#             position_lab = -1\n",
    "#         ob = []\n",
    "#         for i in ob_data.as_matrix().T:\n",
    "#             ob += i.tolist()\n",
    "#         state = ob + [position_lab] + \\\n",
    "#         [self._params['history_data'].tail(1).loc[:,i].values[0] for i in self._params['no_time_var_nm']]\n",
    "#         future_months_price = []\n",
    "#         if self._params.get('reward_step') :\n",
    "#             for i in self._params['reward_step']:\n",
    "#                 future_months_price = future_months_price + [self._params['data'].loc[self._params['iteration'] + i - 1, self._params['reward_col']]]\n",
    "#         else :\n",
    "#             future_months_price = []\n",
    "#         return [state,future_months_price]\n",
    "    \n",
    "    def gen_data(self, data, time):\n",
    "        key = data.columns.tolist()\n",
    "        D = pd.DataFrame()\n",
    "        for key_ in key :\n",
    "            history_key = [] \n",
    "            for j in range(self._params['history_length'],0,-1): \n",
    "                history_key = history_key + [data[key_][time-j]]\n",
    "            D[key_]=history_key\n",
    "        return D\n",
    "    \n",
    "    def reward_function(self, return_list, gamma):\n",
    "        if return_list:\n",
    "            reward = 0\n",
    "            for i in range(len(return_list)-1):\n",
    "                reward += gamma**(i+1)*(1-gamma)*return_list[i]\n",
    "            i += 1\n",
    "            reward += gamma**(i+1)*return_list[i]\n",
    "        else:\n",
    "            reward=0\n",
    "        return reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
